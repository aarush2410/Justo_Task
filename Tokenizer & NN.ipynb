{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46b8b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9376073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(r'C:\\Users\\Aarush Kumar\\Downloads\\Recommendation\\df_posts.csv')\n",
    "likes = pd.read_csv(r'C:\\Users\\Aarush Kumar\\Downloads\\Recommendation\\posts_likes.csv')\n",
    "comments = pd.read_csv(r'C:\\Users\\Aarush Kumar\\Downloads\\Recommendation\\post_comments.csv')\n",
    "grps = pd.read_csv( r'C:\\Users\\Aarush Kumar\\Downloads\\Recommendation\\grp.csv')\n",
    "org = pd.read_csv( r'C:\\Users\\Aarush Kumar\\Downloads\\Recommendation\\org.csv')\n",
    "user = pd.read_csv( r'C:\\Users\\Aarush Kumar\\Downloads\\Recommendation\\user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9662da36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['react_type', 'member_id', 'source_id', 'source_type', 'created_at',\n",
       "       'deleted_at'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2e4c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         20.689655\n",
       "1        100.000000\n",
       "2          0.000000\n",
       "3          0.000000\n",
       "4          0.000000\n",
       "            ...    \n",
       "14917     28.571429\n",
       "14918      0.000000\n",
       "14919    200.000000\n",
       "14920    200.000000\n",
       "14921     50.000000\n",
       "Name: Engagement_Rate, Length: 14922, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts['Engagement_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88eda09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599\n"
     ]
    }
   ],
   "source": [
    "zeros_count = (posts['Engagement_Rate'] == 0).sum()\n",
    "print(zeros_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51382e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_org_country = posts.merge(org[['id', 'country']], \n",
    "                                        left_on=['association_id'], \n",
    "                                        right_on=['id'], \n",
    "                                        how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0df9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_org_and_grp_country = posts_with_org_country.merge(grps[['id', 'organization_id']].merge(org[['id', 'country']], on='id'), \n",
    "                                                             left_on=['association_id', 'association_type'], \n",
    "                                                             right_on=['id', 'organization_id'], \n",
    "                                                             how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3468fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_country(row):\n",
    "    if row['association_type'] == 'organization':\n",
    "        return row['country_x']\n",
    "    elif row['association_type'] == 'group':\n",
    "        return row['country_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e8de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_org_and_grp_country['association_country'] = posts_with_org_and_grp_country.apply(fill_country, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4518e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_org_and_grp_country.drop(columns=['country_x', 'country_y', 'organization_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a1a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_1 = posts_with_org_and_grp_country[['id_x','Engagement_Rate','description','association_type','association_country']]\n",
    "explicit_rating = pd.merge(likes,posts_1,how='inner', left_on='source_id', right_on='id_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfa8276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Praise the Lord'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_1['description'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56fdc674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aarush Kumar\\AppData\\Local\\Temp\\ipykernel_4932\\577771061.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rating.loc[:, 'association_country'] = rating['association_country'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "rating = explicit_rating[['description','member_id','Engagement_Rate','source_id','react_type','association_type','association_country']]\n",
    "rating.loc[:, 'association_country'] = rating['association_country'].astype(str)\n",
    "post = posts[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfc1c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28116 entries, 0 to 28115\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   react_type           28116 non-null  object \n",
      " 1   member_id            28116 non-null  object \n",
      " 2   source_id            28116 non-null  object \n",
      " 3   source_type          28116 non-null  object \n",
      " 4   created_at           28116 non-null  object \n",
      " 5   deleted_at           553 non-null    object \n",
      " 6   id_x                 28116 non-null  object \n",
      " 7   Engagement_Rate      28116 non-null  float64\n",
      " 8   description          28116 non-null  object \n",
      " 9   association_type     28116 non-null  object \n",
      " 10  association_country  27697 non-null  object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "explicit_rating.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa52f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd42f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d9404a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f6b2b141604b84ae2590acc3c127c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aarush Kumar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aarush Kumar\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acf89bb6efb419da12d102226db44e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('ai4bharat/indic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fface246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb911ef31904d8b9648c59f8171fd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aarush Kumar\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('ai4bharat/indic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a15420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sentences = org['title'].tolist()\n",
    "post_sentences = posts['description'].tolist()\n",
    "comment_sentences = comments['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "848792be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = org_sentences + post_sentences + comment_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a288631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AutoModel\n",
    "sentences = all_sentences\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
    "model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
    "word_vectors = []\n",
    "for sentence in sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    word_vectors.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8335ec57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15934"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83604a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.0121, -0.0035, -0.0175,  ..., -0.0262, -0.0208, -0.0218],\n",
      "         [ 0.2402, -0.3323, -0.2352,  ..., -0.0267, -0.0873, -0.1347],\n",
      "         [ 0.5314, -0.3854,  0.0146,  ...,  0.1481,  0.3342, -0.2639],\n",
      "         ...,\n",
      "         [ 1.0052, -0.2547, -0.4842,  ...,  0.1704, -0.0805,  0.1340],\n",
      "         [-0.2866, -0.1035,  0.1761,  ..., -0.2732, -0.1044,  0.1791],\n",
      "         [-0.0121, -0.0035, -0.0175,  ..., -0.0262, -0.0208, -0.0218]]]), pooler_output=tensor([[-0.0506,  0.0527,  0.0218,  0.0023,  0.0487,  0.0852,  0.0380, -0.0052,\n",
      "         -0.0145,  0.0635, -0.0134,  0.0493,  0.0122, -0.0192,  0.0493, -0.0149,\n",
      "         -0.0429,  0.0018, -0.1177, -0.0836,  0.0774,  0.0747, -0.0347, -0.1007,\n",
      "          0.0065, -0.0344, -0.0557, -0.0025,  0.0232,  0.0092,  0.0275, -0.0026,\n",
      "          0.0171,  0.0373,  0.0025, -0.0405, -0.0199, -0.0255,  0.1135,  0.0268,\n",
      "          0.0294, -0.0142, -0.1132,  0.0402, -0.0674, -0.0349,  0.0237, -0.0666,\n",
      "         -0.0485,  0.0072, -0.0543, -0.0166,  0.0194, -0.0283, -0.0226,  0.0559,\n",
      "         -0.0130, -0.0272,  0.0536, -0.0101,  0.0050,  0.0163,  0.0646,  0.1028,\n",
      "          0.0322, -0.0246,  0.0786,  0.1051,  0.0686,  0.0254,  0.0366, -0.0226,\n",
      "          0.0070,  0.1566,  0.0288,  0.0316,  0.0174,  0.0078, -0.0862,  0.0629,\n",
      "          0.0307, -0.0429,  0.0410,  0.0191, -0.0409,  0.0288,  0.0116,  0.0595,\n",
      "         -0.0025, -0.0202,  0.0252, -0.1034,  0.0884,  0.0838,  0.0617,  0.0125,\n",
      "          0.0127, -0.0073, -0.0389,  0.0507, -0.0530,  0.1220, -0.0698,  0.0812,\n",
      "          0.0647, -0.0364,  0.0376, -0.0531,  0.0337,  0.0545, -0.0188,  0.0433,\n",
      "          0.0308, -0.0230, -0.0652,  0.0743, -0.1191, -0.0858, -0.0659,  0.0588,\n",
      "         -0.0121,  0.0620,  0.0107, -0.0672, -0.0793,  0.0599,  0.0840,  0.0973,\n",
      "          0.0323, -0.0157, -0.0462,  0.0316, -0.0181,  0.0367,  0.0908, -0.0546,\n",
      "         -0.0068,  0.0013, -0.0064,  0.0095, -0.0363, -0.0074, -0.0730, -0.0148,\n",
      "         -0.0117, -0.0033, -0.0285, -0.0387, -0.0316,  0.0477,  0.0794,  0.0142,\n",
      "          0.0106, -0.0080, -0.0806, -0.0074,  0.0410, -0.1067,  0.0493,  0.0842,\n",
      "          0.0316,  0.0774, -0.0543,  0.0256,  0.0343, -0.0625, -0.0031, -0.0940,\n",
      "          0.0893, -0.0861,  0.0216,  0.0208,  0.0328, -0.0420, -0.0461, -0.0272,\n",
      "          0.0138, -0.0050, -0.0653, -0.0223, -0.0111,  0.0540,  0.0563,  0.0341,\n",
      "         -0.0597,  0.0165,  0.0489, -0.0646, -0.0028, -0.0833, -0.0747,  0.0674,\n",
      "          0.0461, -0.0800,  0.0148, -0.0073,  0.0358,  0.0199,  0.0962, -0.0571,\n",
      "         -0.0313,  0.0905, -0.0297, -0.0727, -0.0544, -0.0222, -0.0934,  0.0527,\n",
      "         -0.0107,  0.0342,  0.0295, -0.0170,  0.0569,  0.0367, -0.0348, -0.0003,\n",
      "         -0.0654,  0.0320,  0.0396,  0.0370,  0.0004, -0.0943, -0.0368, -0.0455,\n",
      "          0.0079, -0.0275, -0.0866, -0.0344,  0.0073,  0.0091, -0.0441, -0.1502,\n",
      "          0.0369, -0.0249, -0.0747,  0.0050,  0.0072,  0.0291,  0.0346, -0.0087,\n",
      "          0.0344,  0.0058,  0.0071, -0.0208, -0.0677,  0.0640,  0.0148, -0.0986,\n",
      "          0.0516,  0.0086,  0.1241,  0.0333, -0.0392,  0.0365,  0.0571,  0.0735,\n",
      "          0.0417,  0.0366, -0.0115, -0.0464,  0.0266,  0.0112, -0.0388, -0.0039,\n",
      "          0.0373, -0.0266, -0.0157,  0.0568,  0.0389, -0.0580, -0.0648,  0.0768,\n",
      "          0.0749,  0.0440, -0.0239,  0.0809, -0.0188,  0.0534,  0.1134,  0.0582,\n",
      "          0.0648, -0.0682, -0.0848, -0.0577,  0.0560, -0.0102,  0.0604,  0.0429,\n",
      "         -0.0546,  0.0551,  0.0760,  0.0636, -0.0614,  0.0243, -0.0093, -0.0402,\n",
      "          0.1086,  0.0999,  0.0990, -0.0155,  0.0044,  0.0166,  0.0474, -0.0056,\n",
      "         -0.0798, -0.0061, -0.0420, -0.0010, -0.0436,  0.0652,  0.0042,  0.0539,\n",
      "          0.0993, -0.0540,  0.0148,  0.1208, -0.0722,  0.0013, -0.0007,  0.0108,\n",
      "          0.0538, -0.0687, -0.0693, -0.0012, -0.0086,  0.0129,  0.0309, -0.0082,\n",
      "         -0.0313, -0.0815,  0.0562, -0.0064, -0.0733, -0.0182, -0.0239,  0.0184,\n",
      "          0.0114,  0.0540,  0.0419, -0.0127, -0.0695, -0.0431, -0.0431, -0.0292,\n",
      "          0.0302,  0.0560,  0.0078,  0.0127, -0.0530, -0.0577, -0.1013,  0.0504,\n",
      "         -0.0496,  0.0757, -0.0435,  0.0380, -0.0261, -0.1017, -0.0211, -0.0402,\n",
      "          0.0149, -0.0039, -0.0434,  0.0221,  0.0448, -0.0609, -0.0298,  0.1031,\n",
      "          0.0134,  0.0541,  0.0712,  0.0098,  0.0466, -0.1378,  0.0334,  0.0019,\n",
      "          0.0223, -0.0420,  0.0525,  0.0300,  0.0636, -0.0037,  0.0279, -0.0370,\n",
      "         -0.0107, -0.0265,  0.0385,  0.0026,  0.0277,  0.0663, -0.0104,  0.0144,\n",
      "         -0.0677, -0.0023,  0.0132,  0.0360,  0.0897, -0.0380, -0.0041, -0.0021,\n",
      "          0.0123,  0.0278, -0.0097,  0.0379, -0.0254, -0.1475,  0.0403,  0.0907,\n",
      "         -0.0467, -0.0519, -0.0564, -0.0146,  0.0425,  0.0931, -0.0197,  0.0845,\n",
      "          0.0218,  0.0899, -0.0167,  0.0020,  0.0620,  0.0484,  0.0358, -0.0215,\n",
      "          0.0777,  0.0743,  0.0082,  0.0532,  0.0002,  0.0559, -0.0152,  0.0526,\n",
      "          0.0786,  0.0816, -0.0224,  0.0203, -0.0008,  0.0030,  0.0580,  0.1085,\n",
      "         -0.0771,  0.0191,  0.0405, -0.0452, -0.0669, -0.0263,  0.0118, -0.0069,\n",
      "         -0.0272, -0.0792, -0.1072, -0.0199,  0.0915, -0.0561,  0.0070, -0.0518,\n",
      "         -0.1422, -0.0143, -0.0092, -0.0478, -0.0407,  0.0156, -0.0108,  0.0076,\n",
      "          0.0019,  0.0666,  0.0041,  0.0635, -0.0268,  0.0809, -0.1311, -0.0328,\n",
      "          0.0371,  0.0247, -0.0163,  0.0007,  0.0285,  0.0034, -0.0231,  0.0281,\n",
      "          0.0318, -0.0086, -0.0660,  0.0448, -0.0424, -0.0090,  0.0515, -0.0132,\n",
      "         -0.0258, -0.0643, -0.0098, -0.0417,  0.0892, -0.0381,  0.1073, -0.0044,\n",
      "          0.0602, -0.0706,  0.0375, -0.0996, -0.0011, -0.0405, -0.0693,  0.0522,\n",
      "          0.0008, -0.0154, -0.0470,  0.0793, -0.0710,  0.0168,  0.0691, -0.0133,\n",
      "         -0.0293, -0.0687,  0.0509, -0.0711, -0.0085, -0.0527,  0.0432,  0.0282,\n",
      "          0.0261, -0.0196,  0.0248,  0.0166, -0.0222,  0.0083, -0.1303, -0.0415,\n",
      "         -0.0295, -0.0596, -0.0013,  0.0282,  0.0146, -0.0232, -0.0425, -0.0935,\n",
      "         -0.0235, -0.0109, -0.0366,  0.0418,  0.0249,  0.0485,  0.0245,  0.0119,\n",
      "         -0.0160, -0.0351, -0.0658,  0.0662, -0.0154,  0.0132, -0.0514,  0.0466,\n",
      "         -0.0066, -0.0455,  0.1076, -0.0365, -0.0180, -0.0328, -0.0711,  0.0322,\n",
      "         -0.0205, -0.0755,  0.0895, -0.0116,  0.0171,  0.0112,  0.0156,  0.0112,\n",
      "         -0.0402, -0.0420, -0.0469,  0.0072,  0.0542, -0.0295,  0.0385, -0.0189,\n",
      "         -0.0666,  0.0768,  0.0132,  0.0586,  0.0360, -0.0970,  0.0061, -0.0284,\n",
      "          0.1059,  0.0065, -0.0224,  0.0421,  0.0190, -0.0187, -0.0753,  0.0658,\n",
      "          0.0263, -0.0476,  0.0275,  0.0792, -0.0087,  0.0800, -0.0130, -0.0382,\n",
      "          0.0568,  0.0182,  0.0128, -0.0063, -0.0144,  0.0411, -0.0132,  0.0215,\n",
      "          0.0755, -0.0263,  0.0199, -0.0838,  0.0687,  0.0106, -0.0305, -0.0045,\n",
      "         -0.0626, -0.0577, -0.0677,  0.0685, -0.0084, -0.0228, -0.0403, -0.0296,\n",
      "         -0.0515, -0.0041,  0.0366,  0.0415, -0.0691,  0.0233, -0.0025, -0.0589,\n",
      "          0.0124, -0.0229, -0.1021, -0.0093, -0.0268, -0.0007,  0.0022, -0.0747,\n",
      "          0.0032, -0.0244, -0.0580, -0.1114, -0.0826,  0.0148,  0.0296,  0.1338,\n",
      "         -0.0117,  0.0835, -0.0160,  0.0926,  0.0334, -0.0320,  0.0481, -0.0934,\n",
      "          0.0236, -0.0002, -0.0420, -0.1005, -0.0810,  0.0360, -0.0051,  0.0457,\n",
      "          0.0896, -0.1152,  0.0529,  0.0596, -0.0304,  0.0370,  0.0517, -0.0536,\n",
      "          0.0547,  0.0475, -0.0263, -0.0363, -0.0830, -0.0681, -0.0047, -0.0547,\n",
      "         -0.0250, -0.0722,  0.0834, -0.0534, -0.0433, -0.0813, -0.0350, -0.0178,\n",
      "          0.0284,  0.0294, -0.0251,  0.0074,  0.0009,  0.0970,  0.0768, -0.0091,\n",
      "         -0.0146, -0.0553,  0.0524,  0.0483,  0.0739, -0.0059,  0.0080,  0.1279,\n",
      "         -0.0394,  0.0406,  0.0094, -0.0085, -0.0435, -0.0309, -0.0541, -0.1019,\n",
      "          0.0028, -0.0104, -0.0593,  0.0019, -0.0961,  0.0423,  0.0760, -0.0364,\n",
      "         -0.0466,  0.0699,  0.0298,  0.0450, -0.0158,  0.0244,  0.0329,  0.0093,\n",
      "          0.0312, -0.0690, -0.0803,  0.0220,  0.0316, -0.0113,  0.0425,  0.0479,\n",
      "         -0.0164,  0.0442, -0.0379,  0.0607,  0.0323,  0.0618,  0.0409,  0.0123,\n",
      "         -0.0473,  0.0041, -0.0262,  0.0814,  0.0596,  0.0073, -0.0366, -0.0606,\n",
      "         -0.0835, -0.0476,  0.0831,  0.0061,  0.0075, -0.0045,  0.0682, -0.0230,\n",
      "          0.0558,  0.1376, -0.0546, -0.0586, -0.0594, -0.0294,  0.0275,  0.0118]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "text = \" FMG 216 இனி பாவம் செய்யாதே.\"\n",
    "inputs = tokenizer(text,max_length = 30, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64953fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = tokenizer(all_sentences, padding=True, max_length = 30, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12d79dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 77146,  2444,  ...,     0,     0,     0],\n",
       "        [    2, 27889,    32,  ...,     0,     0,     0],\n",
       "        [    2,    33, 44218,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 47525, 48729,  ...,     0,     0,     0],\n",
       "        [    2,    26,  2604,  ...,     0,     0,     0],\n",
       "        [    2, 26695,  2052,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88566122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-recommenders\n",
      "  Obtaining dependency information for tensorflow-recommenders from https://files.pythonhosted.org/packages/d3/91/7f9977f26bc0c94269d3f157710e9f1a112d1af23d4588285d846228ce3c/tensorflow_recommenders-0.7.3-py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_recommenders-0.7.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-recommenders) (1.4.0)\n",
      "Requirement already satisfied: tensorflow>=2.9.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-recommenders) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (2.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.3.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders)\n",
      "  Obtaining dependency information for typing-extensions<4.6.0,>=3.6.6 from https://files.pythonhosted.org/packages/31/25/5abcd82372d3d4a3932e1fa8c3dbf9efac10cc7c0d16e78467460571b404/typing_extensions-4.5.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\aarush kumar\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.2.2)\n",
      "Downloading tensorflow_recommenders-0.7.3-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.2/96.2 kB 5.4 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: typing-extensions, tensorflow-recommenders\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "Successfully installed tensorflow-recommenders-0.7.3 typing-extensions-4.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.104.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic 2.4.2 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.10.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e50c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45d8b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices({\n",
    "    'react_type': rating['react_type'],\n",
    "    'user_id': rating['member_id'],\n",
    "    'source_id': rating['source_id'],\n",
    "    'country' : rating['association_country'],\n",
    "})\n",
    "posts = tf.data.Dataset.from_tensor_slices(post['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70703e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_description = posts.batch(1_000)\n",
    "unique_post_descriptions = np.unique(np.concatenate(list(post_description)))\n",
    "\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "reaction_type = ratings.batch(1_000_000).map(lambda x: x[\"react_type\"])\n",
    "unique_reaction_types = np.unique(np.concatenate(list(reaction_type)))\n",
    "country_type = ratings.batch(1_000_000).map(lambda x: x[\"country\"])\n",
    "unique_country_types = np.unique(np.concatenate(list(reaction_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d40124a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),  # Adjust dimension to 64\n",
    "        ])\n",
    "\n",
    "        \n",
    "        self.reaction_embedding = tf.keras.Sequential([\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_reaction_types, mask_token=None),\n",
    "                tf.keras.layers.Embedding(len(unique_reaction_types) + 1, 32),  # Keep the dimension as 32\n",
    "            ])\n",
    "        \n",
    "        self.country_embedding = tf.keras.Sequential([\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_country_types, mask_token=None),\n",
    "                tf.keras.layers.Embedding(len(unique_reaction_types) + 1, 32),  # Keep the dimension as 32\n",
    "            ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.reaction_embedding(inputs[\"react_type\"]),\n",
    "            self.country_embedding(inputs['country']),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db09c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding queries.\"\"\"\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding queries.\n",
    "\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.embedding_model = deep_UserModel()\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66e9a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10000  \n",
    "title_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "title_vectorizer.adapt(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08ec9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_post_descriptions, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_post_descriptions) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "            title_vectorizer,\n",
    "            tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, titles):\n",
    "        return tf.concat([\n",
    "            self.title_embedding(titles),\n",
    "            self.title_text_embedding(titles),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "957d9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding.\"\"\"\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding.\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.embedding_model = deep_PostModel()\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26512fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_postsearchModel(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel(layer_sizes)\n",
    "        self.candidate_model = CandidateModel(layer_sizes)\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=posts.batch(128).map(lambda x: self.candidate_model(x)),\n",
    "            ),\n",
    "        )\n",
    "    def compute_loss(self, features, training=False):\n",
    "        query_embeddings = self.query_model({\n",
    "            \"user_id\": features[\"user_id\"],\n",
    "            \"react_type\": features[\"react_type\"],\n",
    "            \"country\": features['country'],# Assuming \"react_type\" is provided\n",
    "        })\n",
    "        post_embeddings = self.candidate_model(features[\"source_id\"])\n",
    "        return self.task(query_embeddings, post_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae2ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "train_size = int(len(ratings) * 0.8)\n",
    "test_size = len(ratings) - train_size\n",
    "train = shuffled.take(train_size)\n",
    "test = shuffled.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d8c6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe881273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11/11 [==============================] - 10s 647ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0192 - factorized_top_k/top_5_categorical_accuracy: 0.0260 - factorized_top_k/top_10_categorical_accuracy: 0.0303 - factorized_top_k/top_50_categorical_accuracy: 0.0429 - factorized_top_k/top_100_categorical_accuracy: 0.0531 - loss: 15873.6353 - regularization_loss: 0.0000e+00 - total_loss: 15873.6353\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 8s 696ms/step - factorized_top_k/top_1_categorical_accuracy: 4.0014e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0017 - factorized_top_k/top_10_categorical_accuracy: 0.0034 - factorized_top_k/top_50_categorical_accuracy: 0.0152 - factorized_top_k/top_100_categorical_accuracy: 0.0283 - loss: 15155.9976 - regularization_loss: 0.0000e+00 - total_loss: 15155.9976\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 9s 854ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0013 - factorized_top_k/top_5_categorical_accuracy: 0.0059 - factorized_top_k/top_10_categorical_accuracy: 0.0112 - factorized_top_k/top_50_categorical_accuracy: 0.0412 - factorized_top_k/top_100_categorical_accuracy: 0.0682 - loss: 14363.6126 - regularization_loss: 0.0000e+00 - total_loss: 14363.6126\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 11s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0025 - factorized_top_k/top_5_categorical_accuracy: 0.0102 - factorized_top_k/top_10_categorical_accuracy: 0.0197 - factorized_top_k/top_50_categorical_accuracy: 0.0682 - factorized_top_k/top_100_categorical_accuracy: 0.1160 - loss: 13527.8221 - regularization_loss: 0.0000e+00 - total_loss: 13527.8221\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0047 - factorized_top_k/top_5_categorical_accuracy: 0.0228 - factorized_top_k/top_10_categorical_accuracy: 0.0372 - factorized_top_k/top_50_categorical_accuracy: 0.1103 - factorized_top_k/top_100_categorical_accuracy: 0.1690 - loss: 13021.7649 - regularization_loss: 0.0000e+00 - total_loss: 13021.7649 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0032 - val_factorized_top_k/top_10_categorical_accuracy: 0.0057 - val_factorized_top_k/top_50_categorical_accuracy: 0.0235 - val_factorized_top_k/top_100_categorical_accuracy: 0.0393 - val_loss: 11470.6855 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11470.6855\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 10s 923ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0066 - factorized_top_k/top_5_categorical_accuracy: 0.0315 - factorized_top_k/top_10_categorical_accuracy: 0.0510 - factorized_top_k/top_50_categorical_accuracy: 0.1357 - factorized_top_k/top_100_categorical_accuracy: 0.1929 - loss: 12653.4287 - regularization_loss: 0.0000e+00 - total_loss: 12653.4287\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 10s 886ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0089 - factorized_top_k/top_5_categorical_accuracy: 0.0429 - factorized_top_k/top_10_categorical_accuracy: 0.0656 - factorized_top_k/top_50_categorical_accuracy: 0.1493 - factorized_top_k/top_100_categorical_accuracy: 0.2140 - loss: 12395.0164 - regularization_loss: 0.0000e+00 - total_loss: 12395.0164\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 10s 888ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0114 - factorized_top_k/top_5_categorical_accuracy: 0.0511 - factorized_top_k/top_10_categorical_accuracy: 0.0746 - factorized_top_k/top_50_categorical_accuracy: 0.1633 - factorized_top_k/top_100_categorical_accuracy: 0.2251 - loss: 12189.3210 - regularization_loss: 0.0000e+00 - total_loss: 12189.3210\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 10s 918ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0120 - factorized_top_k/top_5_categorical_accuracy: 0.0554 - factorized_top_k/top_10_categorical_accuracy: 0.0808 - factorized_top_k/top_50_categorical_accuracy: 0.1727 - factorized_top_k/top_100_categorical_accuracy: 0.2335 - loss: 12067.8002 - regularization_loss: 0.0000e+00 - total_loss: 12067.8002\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0143 - factorized_top_k/top_5_categorical_accuracy: 0.0614 - factorized_top_k/top_10_categorical_accuracy: 0.0890 - factorized_top_k/top_50_categorical_accuracy: 0.1765 - factorized_top_k/top_100_categorical_accuracy: 0.2380 - loss: 11938.4658 - regularization_loss: 0.0000e+00 - total_loss: 11938.4658 - val_factorized_top_k/top_1_categorical_accuracy: 3.5562e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0032 - val_factorized_top_k/top_10_categorical_accuracy: 0.0059 - val_factorized_top_k/top_50_categorical_accuracy: 0.0212 - val_factorized_top_k/top_100_categorical_accuracy: 0.0350 - val_loss: 12334.1367 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12334.1367\n",
      "Epoch 11/40\n",
      "11/11 [==============================] - 10s 897ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0145 - factorized_top_k/top_5_categorical_accuracy: 0.0612 - factorized_top_k/top_10_categorical_accuracy: 0.0884 - factorized_top_k/top_50_categorical_accuracy: 0.1771 - factorized_top_k/top_100_categorical_accuracy: 0.2410 - loss: 11851.4380 - regularization_loss: 0.0000e+00 - total_loss: 11851.4380\n",
      "Epoch 12/40\n",
      "11/11 [==============================] - 10s 914ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0162 - factorized_top_k/top_5_categorical_accuracy: 0.0656 - factorized_top_k/top_10_categorical_accuracy: 0.0925 - factorized_top_k/top_50_categorical_accuracy: 0.1886 - factorized_top_k/top_100_categorical_accuracy: 0.2487 - loss: 11788.0414 - regularization_loss: 0.0000e+00 - total_loss: 11788.0414\n",
      "Epoch 13/40\n",
      "11/11 [==============================] - 10s 869ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0157 - factorized_top_k/top_5_categorical_accuracy: 0.0668 - factorized_top_k/top_10_categorical_accuracy: 0.0925 - factorized_top_k/top_50_categorical_accuracy: 0.1817 - factorized_top_k/top_100_categorical_accuracy: 0.2449 - loss: 11728.2846 - regularization_loss: 0.0000e+00 - total_loss: 11728.2846\n",
      "Epoch 14/40\n",
      "11/11 [==============================] - 10s 912ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0170 - factorized_top_k/top_5_categorical_accuracy: 0.0654 - factorized_top_k/top_10_categorical_accuracy: 0.0937 - factorized_top_k/top_50_categorical_accuracy: 0.1889 - factorized_top_k/top_100_categorical_accuracy: 0.2506 - loss: 11660.9830 - regularization_loss: 0.0000e+00 - total_loss: 11660.9830\n",
      "Epoch 15/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0165 - factorized_top_k/top_5_categorical_accuracy: 0.0667 - factorized_top_k/top_10_categorical_accuracy: 0.0934 - factorized_top_k/top_50_categorical_accuracy: 0.1874 - factorized_top_k/top_100_categorical_accuracy: 0.2496 - loss: 11618.2857 - regularization_loss: 0.0000e+00 - total_loss: 11618.2857 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0027 - val_factorized_top_k/top_10_categorical_accuracy: 0.0055 - val_factorized_top_k/top_50_categorical_accuracy: 0.0212 - val_factorized_top_k/top_100_categorical_accuracy: 0.0322 - val_loss: 13024.6855 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13024.6855\n",
      "Epoch 16/40\n",
      "11/11 [==============================] - 10s 890ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0165 - factorized_top_k/top_5_categorical_accuracy: 0.0675 - factorized_top_k/top_10_categorical_accuracy: 0.0971 - factorized_top_k/top_50_categorical_accuracy: 0.1897 - factorized_top_k/top_100_categorical_accuracy: 0.2520 - loss: 11594.2651 - regularization_loss: 0.0000e+00 - total_loss: 11594.2651\n",
      "Epoch 17/40\n",
      "11/11 [==============================] - 10s 895ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0178 - factorized_top_k/top_5_categorical_accuracy: 0.0647 - factorized_top_k/top_10_categorical_accuracy: 0.0931 - factorized_top_k/top_50_categorical_accuracy: 0.1848 - factorized_top_k/top_100_categorical_accuracy: 0.2483 - loss: 11547.3394 - regularization_loss: 0.0000e+00 - total_loss: 11547.3394\n",
      "Epoch 18/40\n",
      "11/11 [==============================] - 10s 943ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0171 - factorized_top_k/top_5_categorical_accuracy: 0.0675 - factorized_top_k/top_10_categorical_accuracy: 0.0957 - factorized_top_k/top_50_categorical_accuracy: 0.1888 - factorized_top_k/top_100_categorical_accuracy: 0.2512 - loss: 11533.9727 - regularization_loss: 0.0000e+00 - total_loss: 11533.9727\n",
      "Epoch 19/40\n",
      "11/11 [==============================] - 10s 889ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0176 - factorized_top_k/top_5_categorical_accuracy: 0.0694 - factorized_top_k/top_10_categorical_accuracy: 0.0975 - factorized_top_k/top_50_categorical_accuracy: 0.1888 - factorized_top_k/top_100_categorical_accuracy: 0.2522 - loss: 11499.1877 - regularization_loss: 0.0000e+00 - total_loss: 11499.1877\n",
      "Epoch 20/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0177 - factorized_top_k/top_5_categorical_accuracy: 0.0686 - factorized_top_k/top_10_categorical_accuracy: 0.0965 - factorized_top_k/top_50_categorical_accuracy: 0.1896 - factorized_top_k/top_100_categorical_accuracy: 0.2528 - loss: 11473.3710 - regularization_loss: 0.0000e+00 - total_loss: 11473.3710 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0037 - val_factorized_top_k/top_10_categorical_accuracy: 0.0057 - val_factorized_top_k/top_50_categorical_accuracy: 0.0210 - val_factorized_top_k/top_100_categorical_accuracy: 0.0334 - val_loss: 13560.1025 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13560.1025\n",
      "Epoch 21/40\n",
      "11/11 [==============================] - 10s 935ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0173 - factorized_top_k/top_5_categorical_accuracy: 0.0687 - factorized_top_k/top_10_categorical_accuracy: 0.0983 - factorized_top_k/top_50_categorical_accuracy: 0.1909 - factorized_top_k/top_100_categorical_accuracy: 0.2528 - loss: 11454.0607 - regularization_loss: 0.0000e+00 - total_loss: 11454.0607\n",
      "Epoch 22/40\n",
      "11/11 [==============================] - 10s 898ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0181 - factorized_top_k/top_5_categorical_accuracy: 0.0694 - factorized_top_k/top_10_categorical_accuracy: 0.0970 - factorized_top_k/top_50_categorical_accuracy: 0.1882 - factorized_top_k/top_100_categorical_accuracy: 0.2533 - loss: 11449.1261 - regularization_loss: 0.0000e+00 - total_loss: 11449.1261\n",
      "Epoch 23/40\n",
      "11/11 [==============================] - 10s 917ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0184 - factorized_top_k/top_5_categorical_accuracy: 0.0695 - factorized_top_k/top_10_categorical_accuracy: 0.0987 - factorized_top_k/top_50_categorical_accuracy: 0.1929 - factorized_top_k/top_100_categorical_accuracy: 0.2567 - loss: 11431.9104 - regularization_loss: 0.0000e+00 - total_loss: 11431.9104\n",
      "Epoch 24/40\n",
      "11/11 [==============================] - 10s 928ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0189 - factorized_top_k/top_5_categorical_accuracy: 0.0699 - factorized_top_k/top_10_categorical_accuracy: 0.0977 - factorized_top_k/top_50_categorical_accuracy: 0.1919 - factorized_top_k/top_100_categorical_accuracy: 0.2546 - loss: 11401.1431 - regularization_loss: 0.0000e+00 - total_loss: 11401.1431\n",
      "Epoch 25/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0180 - factorized_top_k/top_5_categorical_accuracy: 0.0696 - factorized_top_k/top_10_categorical_accuracy: 0.0979 - factorized_top_k/top_50_categorical_accuracy: 0.1932 - factorized_top_k/top_100_categorical_accuracy: 0.2555 - loss: 11397.3026 - regularization_loss: 0.0000e+00 - total_loss: 11397.3026 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0032 - val_factorized_top_k/top_10_categorical_accuracy: 0.0052 - val_factorized_top_k/top_50_categorical_accuracy: 0.0199 - val_factorized_top_k/top_100_categorical_accuracy: 0.0317 - val_loss: 13940.1836 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13940.1836\n",
      "Epoch 26/40\n",
      "11/11 [==============================] - 10s 906ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0184 - factorized_top_k/top_5_categorical_accuracy: 0.0689 - factorized_top_k/top_10_categorical_accuracy: 0.0979 - factorized_top_k/top_50_categorical_accuracy: 0.1924 - factorized_top_k/top_100_categorical_accuracy: 0.2536 - loss: 11370.4215 - regularization_loss: 0.0000e+00 - total_loss: 11370.4215\n",
      "Epoch 27/40\n",
      "11/11 [==============================] - 10s 948ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0190 - factorized_top_k/top_5_categorical_accuracy: 0.0706 - factorized_top_k/top_10_categorical_accuracy: 0.0987 - factorized_top_k/top_50_categorical_accuracy: 0.1942 - factorized_top_k/top_100_categorical_accuracy: 0.2548 - loss: 11352.9097 - regularization_loss: 0.0000e+00 - total_loss: 11352.9097\n",
      "Epoch 28/40\n",
      "11/11 [==============================] - 10s 912ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0178 - factorized_top_k/top_5_categorical_accuracy: 0.0701 - factorized_top_k/top_10_categorical_accuracy: 0.1007 - factorized_top_k/top_50_categorical_accuracy: 0.1939 - factorized_top_k/top_100_categorical_accuracy: 0.2587 - loss: 11343.4949 - regularization_loss: 0.0000e+00 - total_loss: 11343.4949\n",
      "Epoch 29/40\n",
      "11/11 [==============================] - 11s 952ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0184 - factorized_top_k/top_5_categorical_accuracy: 0.0701 - factorized_top_k/top_10_categorical_accuracy: 0.0990 - factorized_top_k/top_50_categorical_accuracy: 0.1916 - factorized_top_k/top_100_categorical_accuracy: 0.2521 - loss: 11323.4765 - regularization_loss: 0.0000e+00 - total_loss: 11323.4765\n",
      "Epoch 30/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0205 - factorized_top_k/top_5_categorical_accuracy: 0.0707 - factorized_top_k/top_10_categorical_accuracy: 0.0998 - factorized_top_k/top_50_categorical_accuracy: 0.1934 - factorized_top_k/top_100_categorical_accuracy: 0.2564 - loss: 11311.9268 - regularization_loss: 0.0000e+00 - total_loss: 11311.9268 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0034 - val_factorized_top_k/top_10_categorical_accuracy: 0.0060 - val_factorized_top_k/top_50_categorical_accuracy: 0.0206 - val_factorized_top_k/top_100_categorical_accuracy: 0.0318 - val_loss: 14270.7012 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14270.7012\n",
      "Epoch 31/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0190 - factorized_top_k/top_5_categorical_accuracy: 0.0720 - factorized_top_k/top_10_categorical_accuracy: 0.0999 - factorized_top_k/top_50_categorical_accuracy: 0.1907 - factorized_top_k/top_100_categorical_accuracy: 0.2539 - loss: 11297.1278 - regularization_loss: 0.0000e+00 - total_loss: 11297.1278\n",
      "Epoch 32/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0198 - factorized_top_k/top_5_categorical_accuracy: 0.0711 - factorized_top_k/top_10_categorical_accuracy: 0.1006 - factorized_top_k/top_50_categorical_accuracy: 0.1940 - factorized_top_k/top_100_categorical_accuracy: 0.2574 - loss: 11297.3420 - regularization_loss: 0.0000e+00 - total_loss: 11297.3420\n",
      "Epoch 33/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0190 - factorized_top_k/top_5_categorical_accuracy: 0.0712 - factorized_top_k/top_10_categorical_accuracy: 0.1003 - factorized_top_k/top_50_categorical_accuracy: 0.1934 - factorized_top_k/top_100_categorical_accuracy: 0.2586 - loss: 11299.4511 - regularization_loss: 0.0000e+00 - total_loss: 11299.4511\n",
      "Epoch 34/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0197 - factorized_top_k/top_5_categorical_accuracy: 0.0705 - factorized_top_k/top_10_categorical_accuracy: 0.1014 - factorized_top_k/top_50_categorical_accuracy: 0.1939 - factorized_top_k/top_100_categorical_accuracy: 0.2558 - loss: 11290.1538 - regularization_loss: 0.0000e+00 - total_loss: 11290.1538\n",
      "Epoch 35/40\n",
      "11/11 [==============================] - 14s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0190 - factorized_top_k/top_5_categorical_accuracy: 0.0710 - factorized_top_k/top_10_categorical_accuracy: 0.1005 - factorized_top_k/top_50_categorical_accuracy: 0.1942 - factorized_top_k/top_100_categorical_accuracy: 0.2574 - loss: 11268.2081 - regularization_loss: 0.0000e+00 - total_loss: 11268.2081 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0028 - val_factorized_top_k/top_10_categorical_accuracy: 0.0059 - val_factorized_top_k/top_50_categorical_accuracy: 0.0210 - val_factorized_top_k/top_100_categorical_accuracy: 0.0336 - val_loss: 14552.3945 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14552.3945\n",
      "Epoch 36/40\n",
      "11/11 [==============================] - 10s 930ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0206 - factorized_top_k/top_5_categorical_accuracy: 0.0717 - factorized_top_k/top_10_categorical_accuracy: 0.1008 - factorized_top_k/top_50_categorical_accuracy: 0.1949 - factorized_top_k/top_100_categorical_accuracy: 0.2579 - loss: 11268.8764 - regularization_loss: 0.0000e+00 - total_loss: 11268.8764\n",
      "Epoch 37/40\n",
      "11/11 [==============================] - 11s 934ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0200 - factorized_top_k/top_5_categorical_accuracy: 0.0730 - factorized_top_k/top_10_categorical_accuracy: 0.1010 - factorized_top_k/top_50_categorical_accuracy: 0.1970 - factorized_top_k/top_100_categorical_accuracy: 0.2598 - loss: 11262.5223 - regularization_loss: 0.0000e+00 - total_loss: 11262.5223\n",
      "Epoch 38/40\n",
      "11/11 [==============================] - 11s 953ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0184 - factorized_top_k/top_5_categorical_accuracy: 0.0702 - factorized_top_k/top_10_categorical_accuracy: 0.0999 - factorized_top_k/top_50_categorical_accuracy: 0.1955 - factorized_top_k/top_100_categorical_accuracy: 0.2584 - loss: 11242.7642 - regularization_loss: 0.0000e+00 - total_loss: 11242.7642\n",
      "Epoch 39/40\n",
      "11/11 [==============================] - 10s 913ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0199 - factorized_top_k/top_5_categorical_accuracy: 0.0718 - factorized_top_k/top_10_categorical_accuracy: 0.1002 - factorized_top_k/top_50_categorical_accuracy: 0.1943 - factorized_top_k/top_100_categorical_accuracy: 0.2585 - loss: 11240.0844 - regularization_loss: 0.0000e+00 - total_loss: 11240.0844\n",
      "Epoch 40/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0185 - factorized_top_k/top_5_categorical_accuracy: 0.0715 - factorized_top_k/top_10_categorical_accuracy: 0.0999 - factorized_top_k/top_50_categorical_accuracy: 0.1934 - factorized_top_k/top_100_categorical_accuracy: 0.2576 - loss: 11227.1563 - regularization_loss: 0.0000e+00 - total_loss: 11227.1563 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0032 - val_factorized_top_k/top_10_categorical_accuracy: 0.0062 - val_factorized_top_k/top_50_categorical_accuracy: 0.0215 - val_factorized_top_k/top_100_categorical_accuracy: 0.0327 - val_loss: 14745.1152 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14745.1152\n",
      "Accuracy: 0.03.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "model = deep_postlevelModel([32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs)\n",
    "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a7c3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11/11 [==============================] - 9s 675ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0188 - factorized_top_k/top_5_categorical_accuracy: 0.0381 - factorized_top_k/top_10_categorical_accuracy: 0.0520 - factorized_top_k/top_50_categorical_accuracy: 0.1274 - factorized_top_k/top_100_categorical_accuracy: 0.1823 - loss: 19002.4803 - regularization_loss: 0.0000e+00 - total_loss: 19002.4803\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 8s 722ms/step - factorized_top_k/top_1_categorical_accuracy: 9.7813e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0036 - factorized_top_k/top_10_categorical_accuracy: 0.0060 - factorized_top_k/top_50_categorical_accuracy: 0.0239 - factorized_top_k/top_100_categorical_accuracy: 0.0425 - loss: 15575.7938 - regularization_loss: 0.0000e+00 - total_loss: 15575.7938\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 10s 941ms/step - factorized_top_k/top_1_categorical_accuracy: 8.8921e-05 - factorized_top_k/top_5_categorical_accuracy: 8.0028e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0016 - factorized_top_k/top_50_categorical_accuracy: 0.0120 - factorized_top_k/top_100_categorical_accuracy: 0.0262 - loss: 15426.7737 - regularization_loss: 0.0000e+00 - total_loss: 15426.7737\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 3.5568e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0019 - factorized_top_k/top_10_categorical_accuracy: 0.0036 - factorized_top_k/top_50_categorical_accuracy: 0.0163 - factorized_top_k/top_100_categorical_accuracy: 0.0345 - loss: 15171.3088 - regularization_loss: 0.0000e+00 - total_loss: 15171.3088\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 16s 1s/step - factorized_top_k/top_1_categorical_accuracy: 7.1136e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0028 - factorized_top_k/top_10_categorical_accuracy: 0.0052 - factorized_top_k/top_50_categorical_accuracy: 0.0207 - factorized_top_k/top_100_categorical_accuracy: 0.0357 - loss: 14965.3402 - regularization_loss: 0.0000e+00 - total_loss: 14965.3402 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 5.3343e-04 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0094 - val_factorized_top_k/top_100_categorical_accuracy: 0.0217 - val_loss: 10846.6699 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10846.6699\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 11s 1000ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0061 - factorized_top_k/top_5_categorical_accuracy: 0.0101 - factorized_top_k/top_10_categorical_accuracy: 0.0134 - factorized_top_k/top_50_categorical_accuracy: 0.0309 - factorized_top_k/top_100_categorical_accuracy: 0.0462 - loss: 14791.1947 - regularization_loss: 0.0000e+00 - total_loss: 14791.1947\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0100 - factorized_top_k/top_5_categorical_accuracy: 0.0146 - factorized_top_k/top_10_categorical_accuracy: 0.0192 - factorized_top_k/top_50_categorical_accuracy: 0.0365 - factorized_top_k/top_100_categorical_accuracy: 0.0559 - loss: 14649.9167 - regularization_loss: 0.0000e+00 - total_loss: 14649.9167\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 11s 946ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0040 - factorized_top_k/top_5_categorical_accuracy: 0.0113 - factorized_top_k/top_10_categorical_accuracy: 0.0155 - factorized_top_k/top_50_categorical_accuracy: 0.0358 - factorized_top_k/top_100_categorical_accuracy: 0.0565 - loss: 14528.0896 - regularization_loss: 0.0000e+00 - total_loss: 14528.0896\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 10s 934ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0094 - factorized_top_k/top_5_categorical_accuracy: 0.0136 - factorized_top_k/top_10_categorical_accuracy: 0.0178 - factorized_top_k/top_50_categorical_accuracy: 0.0379 - factorized_top_k/top_100_categorical_accuracy: 0.0577 - loss: 14427.1367 - regularization_loss: 0.0000e+00 - total_loss: 14427.1367\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 14s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0157 - factorized_top_k/top_5_categorical_accuracy: 0.0219 - factorized_top_k/top_10_categorical_accuracy: 0.0268 - factorized_top_k/top_50_categorical_accuracy: 0.0512 - factorized_top_k/top_100_categorical_accuracy: 0.0738 - loss: 14377.4735 - regularization_loss: 0.0000e+00 - total_loss: 14377.4735 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 5.3343e-04 - val_factorized_top_k/top_10_categorical_accuracy: 0.0021 - val_factorized_top_k/top_50_categorical_accuracy: 0.0091 - val_factorized_top_k/top_100_categorical_accuracy: 0.0196 - val_loss: 11125.9170 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11125.9170\n",
      "Epoch 11/40\n",
      "11/11 [==============================] - 11s 967ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0119 - factorized_top_k/top_5_categorical_accuracy: 0.0183 - factorized_top_k/top_10_categorical_accuracy: 0.0245 - factorized_top_k/top_50_categorical_accuracy: 0.0469 - factorized_top_k/top_100_categorical_accuracy: 0.0681 - loss: 14231.9331 - regularization_loss: 0.0000e+00 - total_loss: 14231.9331\n",
      "Epoch 12/40\n",
      "11/11 [==============================] - 10s 929ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0164 - factorized_top_k/top_5_categorical_accuracy: 0.0254 - factorized_top_k/top_10_categorical_accuracy: 0.0301 - factorized_top_k/top_50_categorical_accuracy: 0.0570 - factorized_top_k/top_100_categorical_accuracy: 0.0807 - loss: 14088.0317 - regularization_loss: 0.0000e+00 - total_loss: 14088.0317\n",
      "Epoch 13/40\n",
      "11/11 [==============================] - 11s 937ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0188 - factorized_top_k/top_5_categorical_accuracy: 0.0284 - factorized_top_k/top_10_categorical_accuracy: 0.0352 - factorized_top_k/top_50_categorical_accuracy: 0.0622 - factorized_top_k/top_100_categorical_accuracy: 0.0823 - loss: 13993.8033 - regularization_loss: 0.0000e+00 - total_loss: 13993.8033\n",
      "Epoch 14/40\n",
      "11/11 [==============================] - 11s 962ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0197 - factorized_top_k/top_5_categorical_accuracy: 0.0289 - factorized_top_k/top_10_categorical_accuracy: 0.0360 - factorized_top_k/top_50_categorical_accuracy: 0.0686 - factorized_top_k/top_100_categorical_accuracy: 0.0940 - loss: 13932.3802 - regularization_loss: 0.0000e+00 - total_loss: 13932.3802\n",
      "Epoch 15/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0137 - factorized_top_k/top_5_categorical_accuracy: 0.0252 - factorized_top_k/top_10_categorical_accuracy: 0.0337 - factorized_top_k/top_50_categorical_accuracy: 0.0693 - factorized_top_k/top_100_categorical_accuracy: 0.0962 - loss: 13866.9364 - regularization_loss: 0.0000e+00 - total_loss: 13866.9364 - val_factorized_top_k/top_1_categorical_accuracy: 5.3343e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0027 - val_factorized_top_k/top_10_categorical_accuracy: 0.0060 - val_factorized_top_k/top_50_categorical_accuracy: 0.0196 - val_factorized_top_k/top_100_categorical_accuracy: 0.0359 - val_loss: 11400.8008 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11400.8008\n",
      "Epoch 16/40\n",
      "11/11 [==============================] - 10s 898ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0154 - factorized_top_k/top_5_categorical_accuracy: 0.0297 - factorized_top_k/top_10_categorical_accuracy: 0.0386 - factorized_top_k/top_50_categorical_accuracy: 0.0741 - factorized_top_k/top_100_categorical_accuracy: 0.1011 - loss: 13765.2118 - regularization_loss: 0.0000e+00 - total_loss: 13765.2118\n",
      "Epoch 17/40\n",
      "11/11 [==============================] - 11s 946ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0165 - factorized_top_k/top_5_categorical_accuracy: 0.0297 - factorized_top_k/top_10_categorical_accuracy: 0.0383 - factorized_top_k/top_50_categorical_accuracy: 0.0800 - factorized_top_k/top_100_categorical_accuracy: 0.1072 - loss: 13674.8841 - regularization_loss: 0.0000e+00 - total_loss: 13674.8841\n",
      "Epoch 18/40\n",
      "11/11 [==============================] - 10s 929ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0257 - factorized_top_k/top_5_categorical_accuracy: 0.0424 - factorized_top_k/top_10_categorical_accuracy: 0.0525 - factorized_top_k/top_50_categorical_accuracy: 0.0948 - factorized_top_k/top_100_categorical_accuracy: 0.1309 - loss: 13653.2480 - regularization_loss: 0.0000e+00 - total_loss: 13653.2480\n",
      "Epoch 19/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0077 - factorized_top_k/top_5_categorical_accuracy: 0.0223 - factorized_top_k/top_10_categorical_accuracy: 0.0345 - factorized_top_k/top_50_categorical_accuracy: 0.0824 - factorized_top_k/top_100_categorical_accuracy: 0.1188 - loss: 13503.5254 - regularization_loss: 0.0000e+00 - total_loss: 13503.5254\n",
      "Epoch 20/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0198 - factorized_top_k/top_5_categorical_accuracy: 0.0400 - factorized_top_k/top_10_categorical_accuracy: 0.0537 - factorized_top_k/top_50_categorical_accuracy: 0.1106 - factorized_top_k/top_100_categorical_accuracy: 0.1495 - loss: 13485.6237 - regularization_loss: 0.0000e+00 - total_loss: 13485.6237 - val_factorized_top_k/top_1_categorical_accuracy: 3.5562e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0018 - val_factorized_top_k/top_10_categorical_accuracy: 0.0044 - val_factorized_top_k/top_50_categorical_accuracy: 0.0231 - val_factorized_top_k/top_100_categorical_accuracy: 0.0402 - val_loss: 11956.6084 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11956.6084\n",
      "Epoch 21/40\n",
      "11/11 [==============================] - 11s 948ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0186 - factorized_top_k/top_5_categorical_accuracy: 0.0338 - factorized_top_k/top_10_categorical_accuracy: 0.0455 - factorized_top_k/top_50_categorical_accuracy: 0.0931 - factorized_top_k/top_100_categorical_accuracy: 0.1288 - loss: 13407.6664 - regularization_loss: 0.0000e+00 - total_loss: 13407.6664\n",
      "Epoch 22/40\n",
      "11/11 [==============================] - 11s 937ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0185 - factorized_top_k/top_5_categorical_accuracy: 0.0381 - factorized_top_k/top_10_categorical_accuracy: 0.0524 - factorized_top_k/top_50_categorical_accuracy: 0.1015 - factorized_top_k/top_100_categorical_accuracy: 0.1412 - loss: 13313.5276 - regularization_loss: 0.0000e+00 - total_loss: 13313.5276\n",
      "Epoch 23/40\n",
      "11/11 [==============================] - 10s 922ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0247 - factorized_top_k/top_5_categorical_accuracy: 0.0450 - factorized_top_k/top_10_categorical_accuracy: 0.0570 - factorized_top_k/top_50_categorical_accuracy: 0.1082 - factorized_top_k/top_100_categorical_accuracy: 0.1425 - loss: 13271.1128 - regularization_loss: 0.0000e+00 - total_loss: 13271.1128\n",
      "Epoch 24/40\n",
      "11/11 [==============================] - 11s 956ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0139 - factorized_top_k/top_5_categorical_accuracy: 0.0391 - factorized_top_k/top_10_categorical_accuracy: 0.0549 - factorized_top_k/top_50_categorical_accuracy: 0.1079 - factorized_top_k/top_100_categorical_accuracy: 0.1420 - loss: 13169.9712 - regularization_loss: 0.0000e+00 - total_loss: 13169.9712\n",
      "Epoch 25/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0209 - factorized_top_k/top_5_categorical_accuracy: 0.0520 - factorized_top_k/top_10_categorical_accuracy: 0.0704 - factorized_top_k/top_50_categorical_accuracy: 0.1313 - factorized_top_k/top_100_categorical_accuracy: 0.1727 - loss: 13145.3867 - regularization_loss: 0.0000e+00 - total_loss: 13145.3867 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0025 - val_factorized_top_k/top_10_categorical_accuracy: 0.0055 - val_factorized_top_k/top_50_categorical_accuracy: 0.0236 - val_factorized_top_k/top_100_categorical_accuracy: 0.0400 - val_loss: 12359.4043 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12359.4043\n",
      "Epoch 26/40\n",
      "11/11 [==============================] - 10s 927ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0280 - factorized_top_k/top_5_categorical_accuracy: 0.0526 - factorized_top_k/top_10_categorical_accuracy: 0.0664 - factorized_top_k/top_50_categorical_accuracy: 0.1195 - factorized_top_k/top_100_categorical_accuracy: 0.1588 - loss: 13138.6479 - regularization_loss: 0.0000e+00 - total_loss: 13138.6479\n",
      "Epoch 27/40\n",
      "11/11 [==============================] - 11s 939ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0149 - factorized_top_k/top_5_categorical_accuracy: 0.0425 - factorized_top_k/top_10_categorical_accuracy: 0.0590 - factorized_top_k/top_50_categorical_accuracy: 0.1211 - factorized_top_k/top_100_categorical_accuracy: 0.1592 - loss: 13050.0880 - regularization_loss: 0.0000e+00 - total_loss: 13050.0880\n",
      "Epoch 28/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0197 - factorized_top_k/top_5_categorical_accuracy: 0.0443 - factorized_top_k/top_10_categorical_accuracy: 0.0603 - factorized_top_k/top_50_categorical_accuracy: 0.1211 - factorized_top_k/top_100_categorical_accuracy: 0.1545 - loss: 12982.8141 - regularization_loss: 0.0000e+00 - total_loss: 12982.8141\n",
      "Epoch 29/40\n",
      "11/11 [==============================] - 11s 946ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0246 - factorized_top_k/top_5_categorical_accuracy: 0.0526 - factorized_top_k/top_10_categorical_accuracy: 0.0683 - factorized_top_k/top_50_categorical_accuracy: 0.1291 - factorized_top_k/top_100_categorical_accuracy: 0.1698 - loss: 12982.5457 - regularization_loss: 0.0000e+00 - total_loss: 12982.5457\n",
      "Epoch 30/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0233 - factorized_top_k/top_5_categorical_accuracy: 0.0517 - factorized_top_k/top_10_categorical_accuracy: 0.0723 - factorized_top_k/top_50_categorical_accuracy: 0.1380 - factorized_top_k/top_100_categorical_accuracy: 0.1791 - loss: 12933.8350 - regularization_loss: 0.0000e+00 - total_loss: 12933.8350 - val_factorized_top_k/top_1_categorical_accuracy: 3.5562e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0028 - val_factorized_top_k/top_10_categorical_accuracy: 0.0064 - val_factorized_top_k/top_50_categorical_accuracy: 0.0261 - val_factorized_top_k/top_100_categorical_accuracy: 0.0464 - val_loss: 12619.4844 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12619.4844\n",
      "Epoch 31/40\n",
      "11/11 [==============================] - 11s 954ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0166 - factorized_top_k/top_5_categorical_accuracy: 0.0467 - factorized_top_k/top_10_categorical_accuracy: 0.0680 - factorized_top_k/top_50_categorical_accuracy: 0.1308 - factorized_top_k/top_100_categorical_accuracy: 0.1678 - loss: 12853.2641 - regularization_loss: 0.0000e+00 - total_loss: 12853.2641\n",
      "Epoch 32/40\n",
      "11/11 [==============================] - 11s 953ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0334 - factorized_top_k/top_5_categorical_accuracy: 0.0625 - factorized_top_k/top_10_categorical_accuracy: 0.0786 - factorized_top_k/top_50_categorical_accuracy: 0.1351 - factorized_top_k/top_100_categorical_accuracy: 0.1701 - loss: 12870.6878 - regularization_loss: 0.0000e+00 - total_loss: 12870.6878\n",
      "Epoch 33/40\n",
      "11/11 [==============================] - 11s 931ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0172 - factorized_top_k/top_5_categorical_accuracy: 0.0464 - factorized_top_k/top_10_categorical_accuracy: 0.0663 - factorized_top_k/top_50_categorical_accuracy: 0.1354 - factorized_top_k/top_100_categorical_accuracy: 0.1774 - loss: 12827.9320 - regularization_loss: 0.0000e+00 - total_loss: 12827.9320\n",
      "Epoch 34/40\n",
      "11/11 [==============================] - 11s 971ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0196 - factorized_top_k/top_5_categorical_accuracy: 0.0552 - factorized_top_k/top_10_categorical_accuracy: 0.0776 - factorized_top_k/top_50_categorical_accuracy: 0.1450 - factorized_top_k/top_100_categorical_accuracy: 0.1813 - loss: 12753.5184 - regularization_loss: 0.0000e+00 - total_loss: 12753.5184\n",
      "Epoch 35/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0277 - factorized_top_k/top_5_categorical_accuracy: 0.0651 - factorized_top_k/top_10_categorical_accuracy: 0.0862 - factorized_top_k/top_50_categorical_accuracy: 0.1467 - factorized_top_k/top_100_categorical_accuracy: 0.1850 - loss: 12732.0836 - regularization_loss: 0.0000e+00 - total_loss: 12732.0836 - val_factorized_top_k/top_1_categorical_accuracy: 3.5562e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0025 - val_factorized_top_k/top_10_categorical_accuracy: 0.0068 - val_factorized_top_k/top_50_categorical_accuracy: 0.0247 - val_factorized_top_k/top_100_categorical_accuracy: 0.0437 - val_loss: 12872.8125 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12872.8125\n",
      "Epoch 36/40\n",
      "11/11 [==============================] - 11s 941ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0253 - factorized_top_k/top_5_categorical_accuracy: 0.0619 - factorized_top_k/top_10_categorical_accuracy: 0.0852 - factorized_top_k/top_50_categorical_accuracy: 0.1519 - factorized_top_k/top_100_categorical_accuracy: 0.1969 - loss: 12704.6071 - regularization_loss: 0.0000e+00 - total_loss: 12704.6071\n",
      "Epoch 37/40\n",
      "11/11 [==============================] - 11s 947ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0324 - factorized_top_k/top_5_categorical_accuracy: 0.0655 - factorized_top_k/top_10_categorical_accuracy: 0.0862 - factorized_top_k/top_50_categorical_accuracy: 0.1491 - factorized_top_k/top_100_categorical_accuracy: 0.1917 - loss: 12738.5614 - regularization_loss: 0.0000e+00 - total_loss: 12738.5614\n",
      "Epoch 38/40\n",
      "11/11 [==============================] - 11s 990ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0196 - factorized_top_k/top_5_categorical_accuracy: 0.0580 - factorized_top_k/top_10_categorical_accuracy: 0.0812 - factorized_top_k/top_50_categorical_accuracy: 0.1454 - factorized_top_k/top_100_categorical_accuracy: 0.1844 - loss: 12635.8929 - regularization_loss: 0.0000e+00 - total_loss: 12635.8929\n",
      "Epoch 39/40\n",
      "11/11 [==============================] - 11s 954ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0206 - factorized_top_k/top_5_categorical_accuracy: 0.0641 - factorized_top_k/top_10_categorical_accuracy: 0.0891 - factorized_top_k/top_50_categorical_accuracy: 0.1553 - factorized_top_k/top_100_categorical_accuracy: 0.1919 - loss: 12596.9307 - regularization_loss: 0.0000e+00 - total_loss: 12596.9307\n",
      "Epoch 40/40\n",
      "11/11 [==============================] - 14s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0240 - factorized_top_k/top_5_categorical_accuracy: 0.0660 - factorized_top_k/top_10_categorical_accuracy: 0.0887 - factorized_top_k/top_50_categorical_accuracy: 0.1542 - factorized_top_k/top_100_categorical_accuracy: 0.1920 - loss: 12573.3017 - regularization_loss: 0.0000e+00 - total_loss: 12573.3017 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0030 - val_factorized_top_k/top_10_categorical_accuracy: 0.0078 - val_factorized_top_k/top_50_categorical_accuracy: 0.0283 - val_factorized_top_k/top_100_categorical_accuracy: 0.0489 - val_loss: 13295.2969 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13295.2969\n",
      "Accuracy: 0.05.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "model = deep_postlevelModel([64,32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "two_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs)\n",
    "accuracy = two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b388bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11/11 [==============================] - 12s 869ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0498 - factorized_top_k/top_5_categorical_accuracy: 0.0612 - factorized_top_k/top_10_categorical_accuracy: 0.0666 - factorized_top_k/top_50_categorical_accuracy: 0.0803 - factorized_top_k/top_100_categorical_accuracy: 0.0934 - loss: 19323.0151 - regularization_loss: 0.0000e+00 - total_loss: 19323.0151\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 10s 872ms/step - factorized_top_k/top_1_categorical_accuracy: 4.4460e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0015 - factorized_top_k/top_10_categorical_accuracy: 0.0026 - factorized_top_k/top_50_categorical_accuracy: 0.0107 - factorized_top_k/top_100_categorical_accuracy: 0.0261 - loss: 15568.7425 - regularization_loss: 0.0000e+00 - total_loss: 15568.7425\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 9s 837ms/step - factorized_top_k/top_1_categorical_accuracy: 2.2230e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0018 - factorized_top_k/top_10_categorical_accuracy: 0.0022 - factorized_top_k/top_50_categorical_accuracy: 0.0090 - factorized_top_k/top_100_categorical_accuracy: 0.0220 - loss: 15560.0479 - regularization_loss: 0.0000e+00 - total_loss: 15560.0479\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 10s 902ms/step - factorized_top_k/top_1_categorical_accuracy: 4.8906e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0015 - factorized_top_k/top_10_categorical_accuracy: 0.0029 - factorized_top_k/top_50_categorical_accuracy: 0.0165 - factorized_top_k/top_100_categorical_accuracy: 0.0336 - loss: 15553.0028 - regularization_loss: 0.0000e+00 - total_loss: 15553.0028\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 8.8921e-05 - factorized_top_k/top_5_categorical_accuracy: 5.3352e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0012 - factorized_top_k/top_50_categorical_accuracy: 0.0065 - factorized_top_k/top_100_categorical_accuracy: 0.0144 - loss: 15531.2161 - regularization_loss: 0.0000e+00 - total_loss: 15531.2161 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_10_categorical_accuracy: 5.3343e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0041 - val_factorized_top_k/top_100_categorical_accuracy: 0.0100 - val_loss: 11211.7188 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11211.7188\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 10s 916ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0016 - factorized_top_k/top_5_categorical_accuracy: 0.0036 - factorized_top_k/top_10_categorical_accuracy: 0.0068 - factorized_top_k/top_50_categorical_accuracy: 0.0289 - factorized_top_k/top_100_categorical_accuracy: 0.0462 - loss: 15478.9724 - regularization_loss: 0.0000e+00 - total_loss: 15478.9724\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 10s 906ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0143 - factorized_top_k/top_5_categorical_accuracy: 0.0260 - factorized_top_k/top_10_categorical_accuracy: 0.0329 - factorized_top_k/top_50_categorical_accuracy: 0.0634 - factorized_top_k/top_100_categorical_accuracy: 0.0840 - loss: 15391.0107 - regularization_loss: 0.0000e+00 - total_loss: 15391.0107\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 11s 946ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0328 - factorized_top_k/top_5_categorical_accuracy: 0.0469 - factorized_top_k/top_10_categorical_accuracy: 0.0541 - factorized_top_k/top_50_categorical_accuracy: 0.0833 - factorized_top_k/top_100_categorical_accuracy: 0.1031 - loss: 15271.1838 - regularization_loss: 0.0000e+00 - total_loss: 15271.1838\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 10s 913ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0237 - factorized_top_k/top_5_categorical_accuracy: 0.0344 - factorized_top_k/top_10_categorical_accuracy: 0.0386 - factorized_top_k/top_50_categorical_accuracy: 0.0577 - factorized_top_k/top_100_categorical_accuracy: 0.0745 - loss: 15157.9433 - regularization_loss: 0.0000e+00 - total_loss: 15157.9433\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 14s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0081 - factorized_top_k/top_5_categorical_accuracy: 0.0213 - factorized_top_k/top_10_categorical_accuracy: 0.0289 - factorized_top_k/top_50_categorical_accuracy: 0.0563 - factorized_top_k/top_100_categorical_accuracy: 0.0744 - loss: 14972.7308 - regularization_loss: 0.0000e+00 - total_loss: 14972.7308 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0012 - val_factorized_top_k/top_10_categorical_accuracy: 0.0018 - val_factorized_top_k/top_50_categorical_accuracy: 0.0108 - val_factorized_top_k/top_100_categorical_accuracy: 0.0219 - val_loss: 11379.8955 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11379.8955\n",
      "Epoch 11/40\n",
      "11/11 [==============================] - 11s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0100 - factorized_top_k/top_5_categorical_accuracy: 0.0243 - factorized_top_k/top_10_categorical_accuracy: 0.0341 - factorized_top_k/top_50_categorical_accuracy: 0.0710 - factorized_top_k/top_100_categorical_accuracy: 0.0970 - loss: 14795.9655 - regularization_loss: 0.0000e+00 - total_loss: 14795.9655\n",
      "Epoch 12/40\n",
      "11/11 [==============================] - 11s 991ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0079 - factorized_top_k/top_5_categorical_accuracy: 0.0237 - factorized_top_k/top_10_categorical_accuracy: 0.0341 - factorized_top_k/top_50_categorical_accuracy: 0.0714 - factorized_top_k/top_100_categorical_accuracy: 0.0995 - loss: 14590.2013 - regularization_loss: 0.0000e+00 - total_loss: 14590.2013\n",
      "Epoch 13/40\n",
      "11/11 [==============================] - 11s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0108 - factorized_top_k/top_5_categorical_accuracy: 0.0304 - factorized_top_k/top_10_categorical_accuracy: 0.0424 - factorized_top_k/top_50_categorical_accuracy: 0.0823 - factorized_top_k/top_100_categorical_accuracy: 0.1107 - loss: 14343.3879 - regularization_loss: 0.0000e+00 - total_loss: 14343.3879\n",
      "Epoch 14/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0105 - factorized_top_k/top_5_categorical_accuracy: 0.0291 - factorized_top_k/top_10_categorical_accuracy: 0.0402 - factorized_top_k/top_50_categorical_accuracy: 0.0830 - factorized_top_k/top_100_categorical_accuracy: 0.1125 - loss: 14215.8115 - regularization_loss: 0.0000e+00 - total_loss: 14215.8115\n",
      "Epoch 15/40\n",
      "11/11 [==============================] - 15s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0122 - factorized_top_k/top_5_categorical_accuracy: 0.0376 - factorized_top_k/top_10_categorical_accuracy: 0.0481 - factorized_top_k/top_50_categorical_accuracy: 0.0949 - factorized_top_k/top_100_categorical_accuracy: 0.1289 - loss: 14026.8893 - regularization_loss: 0.0000e+00 - total_loss: 14026.8893 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0014 - val_factorized_top_k/top_10_categorical_accuracy: 0.0034 - val_factorized_top_k/top_50_categorical_accuracy: 0.0158 - val_factorized_top_k/top_100_categorical_accuracy: 0.0309 - val_loss: 11648.4785 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11648.4785\n",
      "Epoch 16/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0280 - factorized_top_k/top_5_categorical_accuracy: 0.0490 - factorized_top_k/top_10_categorical_accuracy: 0.0609 - factorized_top_k/top_50_categorical_accuracy: 0.1123 - factorized_top_k/top_100_categorical_accuracy: 0.1497 - loss: 13918.2482 - regularization_loss: 0.0000e+00 - total_loss: 13918.2482\n",
      "Epoch 17/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0080 - factorized_top_k/top_5_categorical_accuracy: 0.0226 - factorized_top_k/top_10_categorical_accuracy: 0.0375 - factorized_top_k/top_50_categorical_accuracy: 0.0884 - factorized_top_k/top_100_categorical_accuracy: 0.1301 - loss: 13773.1418 - regularization_loss: 0.0000e+00 - total_loss: 13773.1418\n",
      "Epoch 18/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0264 - factorized_top_k/top_5_categorical_accuracy: 0.0493 - factorized_top_k/top_10_categorical_accuracy: 0.0653 - factorized_top_k/top_50_categorical_accuracy: 0.1246 - factorized_top_k/top_100_categorical_accuracy: 0.1674 - loss: 13622.3364 - regularization_loss: 0.0000e+00 - total_loss: 13622.3364\n",
      "Epoch 19/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0261 - factorized_top_k/top_5_categorical_accuracy: 0.0572 - factorized_top_k/top_10_categorical_accuracy: 0.0750 - factorized_top_k/top_50_categorical_accuracy: 0.1421 - factorized_top_k/top_100_categorical_accuracy: 0.1886 - loss: 13528.3599 - regularization_loss: 0.0000e+00 - total_loss: 13528.3599\n",
      "Epoch 20/40\n",
      "11/11 [==============================] - 15s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0189 - factorized_top_k/top_5_categorical_accuracy: 0.0428 - factorized_top_k/top_10_categorical_accuracy: 0.0625 - factorized_top_k/top_50_categorical_accuracy: 0.1300 - factorized_top_k/top_100_categorical_accuracy: 0.1739 - loss: 13401.0451 - regularization_loss: 0.0000e+00 - total_loss: 13401.0451 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0020 - val_factorized_top_k/top_10_categorical_accuracy: 0.0048 - val_factorized_top_k/top_50_categorical_accuracy: 0.0213 - val_factorized_top_k/top_100_categorical_accuracy: 0.0407 - val_loss: 12142.1816 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12142.1816\n",
      "Epoch 21/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0350 - factorized_top_k/top_5_categorical_accuracy: 0.0719 - factorized_top_k/top_10_categorical_accuracy: 0.0919 - factorized_top_k/top_50_categorical_accuracy: 0.1596 - factorized_top_k/top_100_categorical_accuracy: 0.2086 - loss: 13279.9235 - regularization_loss: 0.0000e+00 - total_loss: 13279.9235\n",
      "Epoch 22/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0334 - factorized_top_k/top_5_categorical_accuracy: 0.0641 - factorized_top_k/top_10_categorical_accuracy: 0.0833 - factorized_top_k/top_50_categorical_accuracy: 0.1479 - factorized_top_k/top_100_categorical_accuracy: 0.1923 - loss: 13247.5825 - regularization_loss: 0.0000e+00 - total_loss: 13247.5825\n",
      "Epoch 23/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0222 - factorized_top_k/top_5_categorical_accuracy: 0.0552 - factorized_top_k/top_10_categorical_accuracy: 0.0751 - factorized_top_k/top_50_categorical_accuracy: 0.1532 - factorized_top_k/top_100_categorical_accuracy: 0.1987 - loss: 13088.5247 - regularization_loss: 0.0000e+00 - total_loss: 13088.5247\n",
      "Epoch 24/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0128 - factorized_top_k/top_5_categorical_accuracy: 0.0458 - factorized_top_k/top_10_categorical_accuracy: 0.0689 - factorized_top_k/top_50_categorical_accuracy: 0.1404 - factorized_top_k/top_100_categorical_accuracy: 0.1873 - loss: 13007.0272 - regularization_loss: 0.0000e+00 - total_loss: 13007.0272\n",
      "Epoch 25/40\n",
      "11/11 [==============================] - 16s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0330 - factorized_top_k/top_5_categorical_accuracy: 0.0702 - factorized_top_k/top_10_categorical_accuracy: 0.0935 - factorized_top_k/top_50_categorical_accuracy: 0.1658 - factorized_top_k/top_100_categorical_accuracy: 0.2101 - loss: 12972.2997 - regularization_loss: 0.0000e+00 - total_loss: 12972.2997 - val_factorized_top_k/top_1_categorical_accuracy: 3.5562e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0011 - val_factorized_top_k/top_10_categorical_accuracy: 0.0032 - val_factorized_top_k/top_50_categorical_accuracy: 0.0240 - val_factorized_top_k/top_100_categorical_accuracy: 0.0418 - val_loss: 12995.6699 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12995.6699\n",
      "Epoch 26/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0187 - factorized_top_k/top_5_categorical_accuracy: 0.0516 - factorized_top_k/top_10_categorical_accuracy: 0.0730 - factorized_top_k/top_50_categorical_accuracy: 0.1466 - factorized_top_k/top_100_categorical_accuracy: 0.1892 - loss: 12864.4581 - regularization_loss: 0.0000e+00 - total_loss: 12864.4581\n",
      "Epoch 27/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0686 - factorized_top_k/top_5_categorical_accuracy: 0.1223 - factorized_top_k/top_10_categorical_accuracy: 0.1481 - factorized_top_k/top_50_categorical_accuracy: 0.2267 - factorized_top_k/top_100_categorical_accuracy: 0.2695 - loss: 12825.4220 - regularization_loss: 0.0000e+00 - total_loss: 12825.4220\n",
      "Epoch 28/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0478 - factorized_top_k/top_5_categorical_accuracy: 0.0886 - factorized_top_k/top_10_categorical_accuracy: 0.1117 - factorized_top_k/top_50_categorical_accuracy: 0.1829 - factorized_top_k/top_100_categorical_accuracy: 0.2264 - loss: 12794.1204 - regularization_loss: 0.0000e+00 - total_loss: 12794.1204\n",
      "Epoch 29/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0286 - factorized_top_k/top_5_categorical_accuracy: 0.0642 - factorized_top_k/top_10_categorical_accuracy: 0.0875 - factorized_top_k/top_50_categorical_accuracy: 0.1665 - factorized_top_k/top_100_categorical_accuracy: 0.2127 - loss: 12731.9597 - regularization_loss: 0.0000e+00 - total_loss: 12731.9597\n",
      "Epoch 30/40\n",
      "11/11 [==============================] - 19s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0266 - factorized_top_k/top_5_categorical_accuracy: 0.0660 - factorized_top_k/top_10_categorical_accuracy: 0.0919 - factorized_top_k/top_50_categorical_accuracy: 0.1739 - factorized_top_k/top_100_categorical_accuracy: 0.2205 - loss: 12708.3543 - regularization_loss: 0.0000e+00 - total_loss: 12708.3543 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0036 - val_factorized_top_k/top_10_categorical_accuracy: 0.0071 - val_factorized_top_k/top_50_categorical_accuracy: 0.0256 - val_factorized_top_k/top_100_categorical_accuracy: 0.0404 - val_loss: 13406.5811 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13406.5811\n",
      "Epoch 31/40\n",
      "11/11 [==============================] - 15s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0216 - factorized_top_k/top_5_categorical_accuracy: 0.0617 - factorized_top_k/top_10_categorical_accuracy: 0.0881 - factorized_top_k/top_50_categorical_accuracy: 0.1710 - factorized_top_k/top_100_categorical_accuracy: 0.2216 - loss: 12611.6785 - regularization_loss: 0.0000e+00 - total_loss: 12611.6785\n",
      "Epoch 32/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0500 - factorized_top_k/top_5_categorical_accuracy: 0.0939 - factorized_top_k/top_10_categorical_accuracy: 0.1188 - factorized_top_k/top_50_categorical_accuracy: 0.1950 - factorized_top_k/top_100_categorical_accuracy: 0.2408 - loss: 12557.0422 - regularization_loss: 0.0000e+00 - total_loss: 12557.0422\n",
      "Epoch 33/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0640 - factorized_top_k/top_5_categorical_accuracy: 0.1130 - factorized_top_k/top_10_categorical_accuracy: 0.1421 - factorized_top_k/top_50_categorical_accuracy: 0.2259 - factorized_top_k/top_100_categorical_accuracy: 0.2695 - loss: 12607.5168 - regularization_loss: 0.0000e+00 - total_loss: 12607.5168\n",
      "Epoch 34/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0329 - factorized_top_k/top_5_categorical_accuracy: 0.0748 - factorized_top_k/top_10_categorical_accuracy: 0.1018 - factorized_top_k/top_50_categorical_accuracy: 0.1837 - factorized_top_k/top_100_categorical_accuracy: 0.2318 - loss: 12520.3477 - regularization_loss: 0.0000e+00 - total_loss: 12520.3477\n",
      "Epoch 35/40\n",
      "11/11 [==============================] - 16s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0190 - factorized_top_k/top_5_categorical_accuracy: 0.0626 - factorized_top_k/top_10_categorical_accuracy: 0.0903 - factorized_top_k/top_50_categorical_accuracy: 0.1734 - factorized_top_k/top_100_categorical_accuracy: 0.2223 - loss: 12410.5151 - regularization_loss: 0.0000e+00 - total_loss: 12410.5151 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0030 - val_factorized_top_k/top_10_categorical_accuracy: 0.0059 - val_factorized_top_k/top_50_categorical_accuracy: 0.0251 - val_factorized_top_k/top_100_categorical_accuracy: 0.0411 - val_loss: 14417.1074 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14417.1074\n",
      "Epoch 36/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0439 - factorized_top_k/top_5_categorical_accuracy: 0.0875 - factorized_top_k/top_10_categorical_accuracy: 0.1134 - factorized_top_k/top_50_categorical_accuracy: 0.1894 - factorized_top_k/top_100_categorical_accuracy: 0.2360 - loss: 12438.2560 - regularization_loss: 0.0000e+00 - total_loss: 12438.2560\n",
      "Epoch 37/40\n",
      "11/11 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0367 - factorized_top_k/top_5_categorical_accuracy: 0.0898 - factorized_top_k/top_10_categorical_accuracy: 0.1191 - factorized_top_k/top_50_categorical_accuracy: 0.2057 - factorized_top_k/top_100_categorical_accuracy: 0.2574 - loss: 12382.8882 - regularization_loss: 0.0000e+00 - total_loss: 12382.8882\n",
      "Epoch 38/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0329 - factorized_top_k/top_5_categorical_accuracy: 0.0837 - factorized_top_k/top_10_categorical_accuracy: 0.1108 - factorized_top_k/top_50_categorical_accuracy: 0.1892 - factorized_top_k/top_100_categorical_accuracy: 0.2346 - loss: 12367.6305 - regularization_loss: 0.0000e+00 - total_loss: 12367.6305\n",
      "Epoch 39/40\n",
      "11/11 [==============================] - 13s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0510 - factorized_top_k/top_5_categorical_accuracy: 0.1054 - factorized_top_k/top_10_categorical_accuracy: 0.1354 - factorized_top_k/top_50_categorical_accuracy: 0.2154 - factorized_top_k/top_100_categorical_accuracy: 0.2629 - loss: 12326.7464 - regularization_loss: 0.0000e+00 - total_loss: 12326.7464\n",
      "Epoch 40/40\n",
      "11/11 [==============================] - 16s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0328 - factorized_top_k/top_5_categorical_accuracy: 0.0743 - factorized_top_k/top_10_categorical_accuracy: 0.0999 - factorized_top_k/top_50_categorical_accuracy: 0.1808 - factorized_top_k/top_100_categorical_accuracy: 0.2339 - loss: 12295.7402 - regularization_loss: 0.0000e+00 - total_loss: 12295.7402 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0044 - val_factorized_top_k/top_10_categorical_accuracy: 0.0071 - val_factorized_top_k/top_50_categorical_accuracy: 0.0249 - val_factorized_top_k/top_100_categorical_accuracy: 0.0445 - val_loss: 14819.6680 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14819.6680\n",
      "Accuracy: 0.04.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "model = deep_postlevelModel([64,32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "three_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs)\n",
    "accuracy = three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a473213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11/11 [==============================] - 18s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0682 - factorized_top_k/top_5_categorical_accuracy: 0.0928 - factorized_top_k/top_10_categorical_accuracy: 0.1054 - factorized_top_k/top_50_categorical_accuracy: 0.1234 - factorized_top_k/top_100_categorical_accuracy: 0.1320 - loss: 17093.5765 - regularization_loss: 0.0000e+00 - total_loss: 17093.5765\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 18s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0026 - factorized_top_k/top_5_categorical_accuracy: 0.0049 - factorized_top_k/top_10_categorical_accuracy: 0.0072 - factorized_top_k/top_50_categorical_accuracy: 0.0185 - factorized_top_k/top_100_categorical_accuracy: 0.0294 - loss: 15557.8401 - regularization_loss: 0.0000e+00 - total_loss: 15557.8401\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 18s 2s/step - factorized_top_k/top_1_categorical_accuracy: 7.1136e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0018 - factorized_top_k/top_10_categorical_accuracy: 0.0038 - factorized_top_k/top_50_categorical_accuracy: 0.0120 - factorized_top_k/top_100_categorical_accuracy: 0.0213 - loss: 15424.3502 - regularization_loss: 0.0000e+00 - total_loss: 15424.3502\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 18s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0117 - factorized_top_k/top_5_categorical_accuracy: 0.0191 - factorized_top_k/top_10_categorical_accuracy: 0.0225 - factorized_top_k/top_50_categorical_accuracy: 0.0428 - factorized_top_k/top_100_categorical_accuracy: 0.0582 - loss: 15075.2720 - regularization_loss: 0.0000e+00 - total_loss: 15075.2720\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0062 - factorized_top_k/top_10_categorical_accuracy: 0.0102 - factorized_top_k/top_50_categorical_accuracy: 0.0368 - factorized_top_k/top_100_categorical_accuracy: 0.0608 - loss: 14649.2593 - regularization_loss: 0.0000e+00 - total_loss: 14649.2593 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0012 - val_factorized_top_k/top_10_categorical_accuracy: 0.0044 - val_factorized_top_k/top_50_categorical_accuracy: 0.0171 - val_factorized_top_k/top_100_categorical_accuracy: 0.0329 - val_loss: 10820.9160 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10820.9160\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 18s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0118 - factorized_top_k/top_5_categorical_accuracy: 0.0207 - factorized_top_k/top_10_categorical_accuracy: 0.0273 - factorized_top_k/top_50_categorical_accuracy: 0.0611 - factorized_top_k/top_100_categorical_accuracy: 0.0939 - loss: 14371.5474 - regularization_loss: 0.0000e+00 - total_loss: 14371.5474\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 18s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0055 - factorized_top_k/top_5_categorical_accuracy: 0.0169 - factorized_top_k/top_10_categorical_accuracy: 0.0236 - factorized_top_k/top_50_categorical_accuracy: 0.0644 - factorized_top_k/top_100_categorical_accuracy: 0.1039 - loss: 14132.0951 - regularization_loss: 0.0000e+00 - total_loss: 14132.0951\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 18s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0151 - factorized_top_k/top_5_categorical_accuracy: 0.0298 - factorized_top_k/top_10_categorical_accuracy: 0.0409 - factorized_top_k/top_50_categorical_accuracy: 0.0898 - factorized_top_k/top_100_categorical_accuracy: 0.1321 - loss: 13908.4255 - regularization_loss: 0.0000e+00 - total_loss: 13908.4255\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 19s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0116 - factorized_top_k/top_5_categorical_accuracy: 0.0318 - factorized_top_k/top_10_categorical_accuracy: 0.0451 - factorized_top_k/top_50_categorical_accuracy: 0.1007 - factorized_top_k/top_100_categorical_accuracy: 0.1462 - loss: 13670.1441 - regularization_loss: 0.0000e+00 - total_loss: 13670.1441\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 25s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0124 - factorized_top_k/top_5_categorical_accuracy: 0.0271 - factorized_top_k/top_10_categorical_accuracy: 0.0398 - factorized_top_k/top_50_categorical_accuracy: 0.0979 - factorized_top_k/top_100_categorical_accuracy: 0.1417 - loss: 13510.4314 - regularization_loss: 0.0000e+00 - total_loss: 13510.4314 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0018 - val_factorized_top_k/top_10_categorical_accuracy: 0.0037 - val_factorized_top_k/top_50_categorical_accuracy: 0.0206 - val_factorized_top_k/top_100_categorical_accuracy: 0.0430 - val_loss: 11324.5059 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11324.5059\n",
      "Epoch 11/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0194 - factorized_top_k/top_5_categorical_accuracy: 0.0426 - factorized_top_k/top_10_categorical_accuracy: 0.0598 - factorized_top_k/top_50_categorical_accuracy: 0.1300 - factorized_top_k/top_100_categorical_accuracy: 0.1795 - loss: 13362.3158 - regularization_loss: 0.0000e+00 - total_loss: 13362.3158\n",
      "Epoch 12/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0175 - factorized_top_k/top_5_categorical_accuracy: 0.0405 - factorized_top_k/top_10_categorical_accuracy: 0.0592 - factorized_top_k/top_50_categorical_accuracy: 0.1299 - factorized_top_k/top_100_categorical_accuracy: 0.1802 - loss: 13164.6453 - regularization_loss: 0.0000e+00 - total_loss: 13164.6453\n",
      "Epoch 13/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0170 - factorized_top_k/top_5_categorical_accuracy: 0.0459 - factorized_top_k/top_10_categorical_accuracy: 0.0671 - factorized_top_k/top_50_categorical_accuracy: 0.1459 - factorized_top_k/top_100_categorical_accuracy: 0.1983 - loss: 13037.7379 - regularization_loss: 0.0000e+00 - total_loss: 13037.7379\n",
      "Epoch 14/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0201 - factorized_top_k/top_5_categorical_accuracy: 0.0493 - factorized_top_k/top_10_categorical_accuracy: 0.0723 - factorized_top_k/top_50_categorical_accuracy: 0.1543 - factorized_top_k/top_100_categorical_accuracy: 0.2038 - loss: 12900.2255 - regularization_loss: 0.0000e+00 - total_loss: 12900.2255\n",
      "Epoch 15/40\n",
      "11/11 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0287 - factorized_top_k/top_5_categorical_accuracy: 0.0592 - factorized_top_k/top_10_categorical_accuracy: 0.0814 - factorized_top_k/top_50_categorical_accuracy: 0.1575 - factorized_top_k/top_100_categorical_accuracy: 0.2101 - loss: 12848.8499 - regularization_loss: 0.0000e+00 - total_loss: 12848.8499 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0037 - val_factorized_top_k/top_10_categorical_accuracy: 0.0078 - val_factorized_top_k/top_50_categorical_accuracy: 0.0288 - val_factorized_top_k/top_100_categorical_accuracy: 0.0480 - val_loss: 11890.7773 - val_regularization_loss: 0.0000e+00 - val_total_loss: 11890.7773\n",
      "Epoch 16/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0305 - factorized_top_k/top_5_categorical_accuracy: 0.0667 - factorized_top_k/top_10_categorical_accuracy: 0.0912 - factorized_top_k/top_50_categorical_accuracy: 0.1679 - factorized_top_k/top_100_categorical_accuracy: 0.2193 - loss: 12700.3495 - regularization_loss: 0.0000e+00 - total_loss: 12700.3495\n",
      "Epoch 17/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0273 - factorized_top_k/top_5_categorical_accuracy: 0.0661 - factorized_top_k/top_10_categorical_accuracy: 0.0888 - factorized_top_k/top_50_categorical_accuracy: 0.1675 - factorized_top_k/top_100_categorical_accuracy: 0.2187 - loss: 12575.7122 - regularization_loss: 0.0000e+00 - total_loss: 12575.7122\n",
      "Epoch 18/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0268 - factorized_top_k/top_5_categorical_accuracy: 0.0673 - factorized_top_k/top_10_categorical_accuracy: 0.0952 - factorized_top_k/top_50_categorical_accuracy: 0.1839 - factorized_top_k/top_100_categorical_accuracy: 0.2361 - loss: 12503.7342 - regularization_loss: 0.0000e+00 - total_loss: 12503.7342\n",
      "Epoch 19/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0263 - factorized_top_k/top_5_categorical_accuracy: 0.0690 - factorized_top_k/top_10_categorical_accuracy: 0.0955 - factorized_top_k/top_50_categorical_accuracy: 0.1844 - factorized_top_k/top_100_categorical_accuracy: 0.2412 - loss: 12416.4863 - regularization_loss: 0.0000e+00 - total_loss: 12416.4863\n",
      "Epoch 20/40\n",
      "11/11 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0225 - factorized_top_k/top_5_categorical_accuracy: 0.0678 - factorized_top_k/top_10_categorical_accuracy: 0.0971 - factorized_top_k/top_50_categorical_accuracy: 0.1853 - factorized_top_k/top_100_categorical_accuracy: 0.2364 - loss: 12310.9481 - regularization_loss: 0.0000e+00 - total_loss: 12310.9481 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0036 - val_factorized_top_k/top_10_categorical_accuracy: 0.0075 - val_factorized_top_k/top_50_categorical_accuracy: 0.0317 - val_factorized_top_k/top_100_categorical_accuracy: 0.0498 - val_loss: 12564.1475 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12564.1475\n",
      "Epoch 21/40\n",
      "11/11 [==============================] - 19s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0424 - factorized_top_k/top_5_categorical_accuracy: 0.0880 - factorized_top_k/top_10_categorical_accuracy: 0.1140 - factorized_top_k/top_50_categorical_accuracy: 0.1957 - factorized_top_k/top_100_categorical_accuracy: 0.2472 - loss: 12301.9887 - regularization_loss: 0.0000e+00 - total_loss: 12301.9887\n",
      "Epoch 22/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0280 - factorized_top_k/top_5_categorical_accuracy: 0.0706 - factorized_top_k/top_10_categorical_accuracy: 0.0979 - factorized_top_k/top_50_categorical_accuracy: 0.1830 - factorized_top_k/top_100_categorical_accuracy: 0.2388 - loss: 12194.4683 - regularization_loss: 0.0000e+00 - total_loss: 12194.4683\n",
      "Epoch 23/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0382 - factorized_top_k/top_5_categorical_accuracy: 0.0865 - factorized_top_k/top_10_categorical_accuracy: 0.1107 - factorized_top_k/top_50_categorical_accuracy: 0.1922 - factorized_top_k/top_100_categorical_accuracy: 0.2415 - loss: 12153.6830 - regularization_loss: 0.0000e+00 - total_loss: 12153.6830\n",
      "Epoch 24/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0275 - factorized_top_k/top_5_categorical_accuracy: 0.0742 - factorized_top_k/top_10_categorical_accuracy: 0.1012 - factorized_top_k/top_50_categorical_accuracy: 0.1858 - factorized_top_k/top_100_categorical_accuracy: 0.2388 - loss: 12102.1927 - regularization_loss: 0.0000e+00 - total_loss: 12102.1927\n",
      "Epoch 25/40\n",
      "11/11 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0301 - factorized_top_k/top_5_categorical_accuracy: 0.0803 - factorized_top_k/top_10_categorical_accuracy: 0.1082 - factorized_top_k/top_50_categorical_accuracy: 0.1935 - factorized_top_k/top_100_categorical_accuracy: 0.2471 - loss: 12037.7320 - regularization_loss: 0.0000e+00 - total_loss: 12037.7320 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0036 - val_factorized_top_k/top_10_categorical_accuracy: 0.0085 - val_factorized_top_k/top_50_categorical_accuracy: 0.0286 - val_factorized_top_k/top_100_categorical_accuracy: 0.0471 - val_loss: 13101.7871 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13101.7871\n",
      "Epoch 26/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0321 - factorized_top_k/top_5_categorical_accuracy: 0.0847 - factorized_top_k/top_10_categorical_accuracy: 0.1135 - factorized_top_k/top_50_categorical_accuracy: 0.2006 - factorized_top_k/top_100_categorical_accuracy: 0.2512 - loss: 12028.4420 - regularization_loss: 0.0000e+00 - total_loss: 12028.4420\n",
      "Epoch 27/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0385 - factorized_top_k/top_5_categorical_accuracy: 0.0901 - factorized_top_k/top_10_categorical_accuracy: 0.1202 - factorized_top_k/top_50_categorical_accuracy: 0.2035 - factorized_top_k/top_100_categorical_accuracy: 0.2528 - loss: 11990.1456 - regularization_loss: 0.0000e+00 - total_loss: 11990.1456\n",
      "Epoch 28/40\n",
      "11/11 [==============================] - 19s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0348 - factorized_top_k/top_5_categorical_accuracy: 0.0824 - factorized_top_k/top_10_categorical_accuracy: 0.1095 - factorized_top_k/top_50_categorical_accuracy: 0.1972 - factorized_top_k/top_100_categorical_accuracy: 0.2489 - loss: 11943.5400 - regularization_loss: 0.0000e+00 - total_loss: 11943.5400\n",
      "Epoch 29/40\n",
      "11/11 [==============================] - 19s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0226 - factorized_top_k/top_5_categorical_accuracy: 0.0708 - factorized_top_k/top_10_categorical_accuracy: 0.1006 - factorized_top_k/top_50_categorical_accuracy: 0.1903 - factorized_top_k/top_100_categorical_accuracy: 0.2464 - loss: 11883.8818 - regularization_loss: 0.0000e+00 - total_loss: 11883.8818\n",
      "Epoch 30/40\n",
      "11/11 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0390 - factorized_top_k/top_5_categorical_accuracy: 0.0940 - factorized_top_k/top_10_categorical_accuracy: 0.1249 - factorized_top_k/top_50_categorical_accuracy: 0.2167 - factorized_top_k/top_100_categorical_accuracy: 0.2695 - loss: 11871.4552 - regularization_loss: 0.0000e+00 - total_loss: 11871.4552 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0044 - val_factorized_top_k/top_10_categorical_accuracy: 0.0071 - val_factorized_top_k/top_50_categorical_accuracy: 0.0272 - val_factorized_top_k/top_100_categorical_accuracy: 0.0429 - val_loss: 13487.2031 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13487.2031\n",
      "Epoch 31/40\n",
      "11/11 [==============================] - 19s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0267 - factorized_top_k/top_5_categorical_accuracy: 0.0788 - factorized_top_k/top_10_categorical_accuracy: 0.1061 - factorized_top_k/top_50_categorical_accuracy: 0.1922 - factorized_top_k/top_100_categorical_accuracy: 0.2461 - loss: 11837.1232 - regularization_loss: 0.0000e+00 - total_loss: 11837.1232\n",
      "Epoch 32/40\n",
      "11/11 [==============================] - 19s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0335 - factorized_top_k/top_5_categorical_accuracy: 0.0869 - factorized_top_k/top_10_categorical_accuracy: 0.1171 - factorized_top_k/top_50_categorical_accuracy: 0.2049 - factorized_top_k/top_100_categorical_accuracy: 0.2552 - loss: 11813.8681 - regularization_loss: 0.0000e+00 - total_loss: 11813.8681\n",
      "Epoch 33/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0376 - factorized_top_k/top_5_categorical_accuracy: 0.0901 - factorized_top_k/top_10_categorical_accuracy: 0.1191 - factorized_top_k/top_50_categorical_accuracy: 0.2129 - factorized_top_k/top_100_categorical_accuracy: 0.2695 - loss: 11808.6597 - regularization_loss: 0.0000e+00 - total_loss: 11808.6597\n",
      "Epoch 34/40\n",
      "11/11 [==============================] - 22s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0385 - factorized_top_k/top_5_categorical_accuracy: 0.0885 - factorized_top_k/top_10_categorical_accuracy: 0.1143 - factorized_top_k/top_50_categorical_accuracy: 0.2032 - factorized_top_k/top_100_categorical_accuracy: 0.2552 - loss: 11815.5873 - regularization_loss: 0.0000e+00 - total_loss: 11815.5873\n",
      "Epoch 35/40\n",
      "11/11 [==============================] - 25s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0253 - factorized_top_k/top_5_categorical_accuracy: 0.0782 - factorized_top_k/top_10_categorical_accuracy: 0.1091 - factorized_top_k/top_50_categorical_accuracy: 0.1988 - factorized_top_k/top_100_categorical_accuracy: 0.2498 - loss: 11726.1736 - regularization_loss: 0.0000e+00 - total_loss: 11726.1736 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0048 - val_factorized_top_k/top_10_categorical_accuracy: 0.0078 - val_factorized_top_k/top_50_categorical_accuracy: 0.0272 - val_factorized_top_k/top_100_categorical_accuracy: 0.0423 - val_loss: 13924.0430 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13924.0430\n",
      "Epoch 36/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0314 - factorized_top_k/top_5_categorical_accuracy: 0.0870 - factorized_top_k/top_10_categorical_accuracy: 0.1139 - factorized_top_k/top_50_categorical_accuracy: 0.2008 - factorized_top_k/top_100_categorical_accuracy: 0.2571 - loss: 11714.2774 - regularization_loss: 0.0000e+00 - total_loss: 11714.2774\n",
      "Epoch 37/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0417 - factorized_top_k/top_5_categorical_accuracy: 0.0960 - factorized_top_k/top_10_categorical_accuracy: 0.1228 - factorized_top_k/top_50_categorical_accuracy: 0.2031 - factorized_top_k/top_100_categorical_accuracy: 0.2513 - loss: 11717.0111 - regularization_loss: 0.0000e+00 - total_loss: 11717.0111\n",
      "Epoch 38/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0290 - factorized_top_k/top_5_categorical_accuracy: 0.0843 - factorized_top_k/top_10_categorical_accuracy: 0.1130 - factorized_top_k/top_50_categorical_accuracy: 0.2031 - factorized_top_k/top_100_categorical_accuracy: 0.2565 - loss: 11690.5927 - regularization_loss: 0.0000e+00 - total_loss: 11690.5927\n",
      "Epoch 39/40\n",
      "11/11 [==============================] - 20s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0348 - factorized_top_k/top_5_categorical_accuracy: 0.0884 - factorized_top_k/top_10_categorical_accuracy: 0.1169 - factorized_top_k/top_50_categorical_accuracy: 0.2038 - factorized_top_k/top_100_categorical_accuracy: 0.2544 - loss: 11680.2442 - regularization_loss: 0.0000e+00 - total_loss: 11680.2442\n",
      "Epoch 40/40\n",
      "11/11 [==============================] - 24s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0334 - factorized_top_k/top_5_categorical_accuracy: 0.0868 - factorized_top_k/top_10_categorical_accuracy: 0.1168 - factorized_top_k/top_50_categorical_accuracy: 0.2097 - factorized_top_k/top_100_categorical_accuracy: 0.2637 - loss: 11656.3647 - regularization_loss: 0.0000e+00 - total_loss: 11656.3647 - val_factorized_top_k/top_1_categorical_accuracy: 1.7781e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0030 - val_factorized_top_k/top_10_categorical_accuracy: 0.0060 - val_factorized_top_k/top_50_categorical_accuracy: 0.0258 - val_factorized_top_k/top_100_categorical_accuracy: 0.0425 - val_loss: 14228.5352 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14228.5352\n",
      "Accuracy: 0.04.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "model = deep_postlevelModel([64,32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "four_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs)\n",
    "\n",
    "accuracy = three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Accuracy: {accuracy:.2f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacc1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
